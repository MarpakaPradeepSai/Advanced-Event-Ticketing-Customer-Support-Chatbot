{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4ETp2s84ykW",
        "outputId": "da496aaa-b6b6-4570-eb2f-864b4004a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    GPT2Tokenizer, GPT2LMHeadModel\n",
        ")\n",
        "from transformers.generation.streamers import TextStreamer\n",
        "\n",
        "# =============================\n",
        "# 1. Load Binary Classification Model (OOD: Yes/No)\n",
        "# =============================\n",
        "classification_model_path = '/content/drive/MyDrive/Chatbot_Query_Classifier/chatbot_query_classifier_distilbert_model'\n",
        "clf_tokenizer = AutoTokenizer.from_pretrained(classification_model_path)\n",
        "clf_model = AutoModelForSequenceClassification.from_pretrained(classification_model_path)\n",
        "clf_model.eval()\n",
        "clf_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clf_model.to(clf_device)\n",
        "\n",
        "# =============================\n",
        "# 2. Load GPT-2 Chatbot Model\n",
        "# =============================\n",
        "gpt2_model_path = '/content/drive/MyDrive/Chatbot/Final_Advanced_event_ticketing_DistilGPT2_fine-tuned'\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_path)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_path)\n",
        "gpt2_model.eval()\n",
        "gpt2_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpt2_model.to(gpt2_device)\n",
        "\n",
        "# =============================\n",
        "# 3. Random OOD Fallback Responses\n",
        "# =============================\n",
        "fallback_responses = [\n",
        "    \"I‚Äôm sorry, but I am unable to assist with this request. If you need help regarding event tickets, I‚Äôd be happy to support you.\",\n",
        "    \"Apologies, but I am not able to provide assistance on this matter. Please let me know if you require help with event tickets.\",\n",
        "    \"Unfortunately, I cannot assist with this. However, I am here to help with any event ticket-related concerns you may have.\",\n",
        "    \"Regrettably, I am unable to assist with this request. If there's anything I can do regarding event tickets, feel free to ask.\",\n",
        "    \"I regret that I am unable to assist in this case. Please reach out if you need support related to event tickets.\",\n",
        "    \"Apologies, but this falls outside the scope of my support. I‚Äôm here if you need any help with event ticket issues.\",\n",
        "    \"I'm sorry, but I cannot assist with this particular topic. If you have questions about event tickets, I‚Äôd be glad to help.\",\n",
        "    \"I regret that I‚Äôm unable to provide assistance here. Please let me know how I can support you with event ticket matters.\",\n",
        "    \"Unfortunately, I am not equipped to assist with this. If you need help with event tickets, I am here for that.\",\n",
        "    \"I apologize, but I cannot help with this request. However, I‚Äôd be happy to assist with anything related to event tickets.\",\n",
        "    \"I‚Äôm sorry, but I‚Äôm unable to support this request. If it‚Äôs about event tickets, I‚Äôll gladly help however I can.\",\n",
        "    \"This matter falls outside the assistance I can offer. Please let me know if you need help with event ticket-related inquiries.\",\n",
        "    \"Regrettably, this is not something I can assist with. I‚Äôm happy to help with any event ticket questions you may have.\",\n",
        "    \"I‚Äôm unable to provide support for this issue. However, I can assist with concerns regarding event tickets.\",\n",
        "    \"I apologize, but I cannot help with this matter. If your inquiry is related to event tickets, I‚Äôd be more than happy to assist.\",\n",
        "    \"I regret that I am unable to offer help in this case. I am, however, available for any event ticket-related questions.\",\n",
        "    \"Unfortunately, I‚Äôm not able to assist with this. Please let me know if there‚Äôs anything I can do regarding event tickets.\",\n",
        "    \"I'm sorry, but I cannot assist with this topic. However, I‚Äôm here to help with any event ticket concerns you may have.\",\n",
        "    \"Apologies, but this request falls outside of my support scope. If you need help with event tickets, I‚Äôm happy to assist.\",\n",
        "    \"I‚Äôm afraid I can‚Äôt help with this matter. If there‚Äôs anything related to event tickets you need, feel free to reach out.\",\n",
        "    \"This is beyond what I can assist with at the moment. Let me know if there‚Äôs anything I can do to help with event tickets.\",\n",
        "    \"Sorry, I‚Äôm unable to provide support on this issue. However, I‚Äôd be glad to assist with event ticket-related topics.\",\n",
        "    \"Apologies, but I can‚Äôt assist with this. Please let me know if you have any event ticket inquiries I can help with.\",\n",
        "    \"I‚Äôm unable to help with this matter. However, if you need assistance with event tickets, I‚Äôm here for you.\",\n",
        "    \"Unfortunately, I can‚Äôt support this request. I‚Äôd be happy to assist with anything related to event tickets instead.\",\n",
        "    \"I‚Äôm sorry, but I can‚Äôt help with this. If your concern is related to event tickets, I‚Äôll do my best to assist.\",\n",
        "    \"Apologies, but this issue is outside of my capabilities. However, I‚Äôm available to help with event ticket-related requests.\",\n",
        "    \"I regret that I cannot assist with this particular matter. Please let me know how I can support you regarding event tickets.\",\n",
        "    \"I‚Äôm sorry, but I‚Äôm not able to help in this instance. I am, however, ready to assist with any questions about event tickets.\",\n",
        "    \"Unfortunately, I‚Äôm unable to help with this topic. Let me know if there's anything event ticket-related I can support you with.\"\n",
        "]\n",
        "\n",
        "# =============================\n",
        "# 4. Optional Streamer to Trim Whitespace\n",
        "# =============================\n",
        "class TrimLeadingTextStreamer(TextStreamer):\n",
        "    def __init__(self, tokenizer, skip_prompt=True, **kwargs):\n",
        "        try:\n",
        "            super().__init__(tokenizer, skip_prompt=skip_prompt, **kwargs)\n",
        "        except TypeError:\n",
        "            decode_kwargs = kwargs.pop(\"decode_kwargs\", {})\n",
        "            decode_kwargs.setdefault(\"skip_special_tokens\", True)\n",
        "            super().__init__(tokenizer, skip_prompt=skip_prompt, decode_kwargs=decode_kwargs)\n",
        "        self._started = False\n",
        "\n",
        "    def on_finalized_text(self, text, stream_end=False):\n",
        "        if not self._started:\n",
        "            text = text.lstrip()\n",
        "            if text:\n",
        "                self._started = True\n",
        "            else:\n",
        "                return\n",
        "        return super().on_finalized_text(text, stream_end=stream_end)\n",
        "\n",
        "# =============================\n",
        "# 5. Classify Whether Input is OOD\n",
        "# =============================\n",
        "def is_ood(query: str) -> bool:\n",
        "    inputs = clf_tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
        "    inputs = {k: v.to(clf_device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = clf_model(**inputs)\n",
        "    pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return pred_id == 1  # True if OOD\n",
        "\n",
        "# =============================\n",
        "# 6. Generate Response with GPT-2 (LIVE Streaming!)\n",
        "# =============================\n",
        "def generate_response_live(instruction: str, max_length=256):\n",
        "    prompt = f\"Instruction: {instruction} Response:\"\n",
        "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\", padding=True).to(gpt2_device)\n",
        "\n",
        "    print(\"ü§ñ Bot: \", end=\"\", flush=True)  # Prefix bot response\n",
        "\n",
        "    streamer = TrimLeadingTextStreamer(gpt2_tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gpt2_model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.5,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=gpt2_tokenizer.eos_token_id,\n",
        "            streamer=streamer  # üî¥ This will print live!\n",
        "        )"
      ],
      "metadata": {
        "id": "9PH9F2kW3GTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# ‚ùáÔ∏è Start Interaction\n",
        "# =============================\n",
        "\n",
        "print(\"\\nü§ñ Welcome to the Event Ticketing Assistant!\\n\")\n",
        "user_input = input(\"üîπ You: \").strip()\n",
        "\n",
        "# Exit early if input is empty\n",
        "if not user_input:\n",
        "    print(\"‚ö†Ô∏è Please enter a query.\")\n",
        "else:\n",
        "    # Capitalize the first letter for better GPT-2 formatting\n",
        "    formatted_input = user_input[0].upper() + user_input[1:] if user_input else \"\"\n",
        "\n",
        "    if is_ood(user_input):\n",
        "        bot_response = random.choice(fallback_responses)\n",
        "        print(f\"ü§ñ Bot: {bot_response}\")\n",
        "    else:\n",
        "        generate_response_live(formatted_input)  # üî¥ Live response only, nothing printed afterwards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dMI5Lrx1LHV",
        "outputId": "c5dbf6dd-40c1-443d-c425-ce2478c70437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Welcome to the Event Ticketing Assistant!\n",
            "\n",
            "üîπ You: How to sell my ticket\n",
            "ü§ñ Bot: Beginning the process of selling or exchanging your event ticket is simple. Kindly adhere to the following instructions:\n",
            "\n",
            "1. Go to {{WEBSITE_URL}} and enter your credentials to log in.\n",
            "2. Proceed to the {{TICKET_SECTION}} area.\n",
            "3. Identify and choose the ticket you wish to sell or exchange.\n",
            "4. Click on the {{SELL_TICKET_OPTION}} button.\n",
            "5. Fill in the necessary details and confirm your choice.\n",
            "\n",
            "By adhering to these steps, you can efficiently handle your event tickets. If any issues arise, do not hesitate to seek further support.\n"
          ]
        }
      ]
    }
  ]
}
